{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](00.00-Learning-ML.ipynb#Table-of-Contents) &bull; [&larr; *Chapter 2.03 - k-Nearest Neighbours*](02.03-k-Nearest-Neighbours.ipynb) &bull; [*Chapter 2.05 - ?* &rarr;](02.05-?.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "# Chapter 2.04 - Decision Trees\n",
    "\n",
    "Decision trees are a family of related algorithms which build logical trees to represent data. Once constructed, a tree resembles a flow chart, starting with a single node or decision point, with two or more branches as optional paths. At each decision point, some feature is considered, and the correct branch is selected based on the value of the feature in the context of the logic presented.\n",
    "\n",
    "For example, the starting node may consider feature $F_1$, with two branches: $F_1 < 4$ and $F_1 >= 4$. If the observation's $F_1$ value is 2, then the first branch is followed. If the observation's $F_1$ value is 27 then the second branch, and so on.\n",
    "\n",
    "Depending on the branch taken, another decision point may be reached, most likely for a different feature. Sub-branches will typically consider different features even at similar depths (that is, the number of decisions made), but will vary based on the parameters specified for the model to decide how and when to create a new branch.\n",
    "\n",
    "When the model stops branching, instead of a decision point we have a node (called a leaf) which contains the predicted label for classification. We can take this value as our classifier output for a given new observation.\n",
    "\n",
    "Decision trees allow us to consider complex relationships between features (unlike Naive Bayes - which assumes each feature is independent), while allowing us to train a model (producing a decision tree from data) for far quicker predictions than achievable with the k-Nearest Neighbours model! \n",
    "\n",
    "As a logical structure, decision trees are also straightforward to inspect, reason about and explain! The trade off is that this simplicity is prone to over-fitting - when we create a model that is able to reproduce its training predictions with low error, but has a high error rate on unseen data. In this chapter, we will see one method for resolving this for a given tree.\n",
    "\n",
    "## Forming a tree\n",
    "\n",
    "Splitting strategy, continuous and categorical data\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "training error > testing error\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "as a method to combat overfitting while making the most of the data\n",
    "\n",
    "## Feature Reduction\n",
    "\n",
    "by determining most important variables from the tree structure\n",
    "\n",
    "## Implementing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Table of Contents](00.00-Learning-ML.ipynb#Table-of-Contents) &bull; [&larr; *Chapter 2.03 - k-Nearest Neighbours*](02.03-k-Nearest-Neighbours.ipynb) &bull; [*Chapter 2.05 - ?* &rarr;](02.05-?.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
