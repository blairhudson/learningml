{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](00.00-Learning-ML.ipynb#Table-of-Contents) &bull; [&larr; *Chapter 2.03 - k-Nearest Neighbours*](02.03-k-Nearest-Neighbours.ipynb) &bull; [*Chapter 2.05 - ?* &rarr;](02.05-?.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "# Chapter 2.04 - Decision Trees\n",
    "\n",
    "Decision trees are a family of related algorithms which build logical trees to represent data. Once constructed, a tree resembles a flow chart, starting with a single node or decision point, with two or more branches as optional paths. At each decision point, some feature is considered, and the correct branch is selected based on the value of the feature in the context of the logic presented.\n",
    "\n",
    "For example, the starting node may consider feature $F_1$, with two branches: $F_1 < 4$ and $F_1 >= 4$. If the observation's $F_1$ value is 2, then the first branch is followed. If the observation's $F_1$ value is 27 then the second branch, and so on.\n",
    "\n",
    "Depending on the branch taken, another decision point may be reached, most likely for a different feature. Sub-branches will typically consider different features even at similar depths (that is, the number of decisions made), but will vary based on the parameters specified for the model to decide how and when to create a new branch.\n",
    "\n",
    "When the model stops branching, instead of a decision point we have a node (called a leaf) which contains the predicted label for classification. We can take this value as our classifier output for a given new observation.\n",
    "\n",
    "Decision trees allow us to consider complex relationships between features (unlike Naive Bayes - which assumes each feature is independent), while allowing us to train a model (producing a decision tree from data) for far quicker predictions than achievable with the k-Nearest Neighbours model! \n",
    "\n",
    "As a logical structure, decision trees are also straightforward to inspect, reason about and explain! The trade off is that this simplicity is prone to over-fitting - when we create a model that is able to reproduce its training predictions with low error, but has a high error rate on unseen data. In this chapter, we will see one method for resolving this for a given tree.\n",
    "\n",
    "## Forming a tree\n",
    "\n",
    "Building a decision tree on given dataset follows a simple, iterative (actually recursive) process. As with the previous classifiers, we start with a dataset which contains multiple features (X's) and set of labels (Y). To build a tree, we want to find the best X to select as the root node to create a split. There are a few strategies for making this selection, but we will focus on one that uses *Gini Impurity*, measures the misclassification of splitting on a split. We want to select the split that minimises misclassification.\n",
    "\n",
    "Let's revisit an example from when we introduced Naive Bayes classifiers:\n",
    "\n",
    "| Employment | Default | Count |\n",
    "|---|---|---|\n",
    "| FT | N | 59 |\n",
    "| FT | Y | 1 |\n",
    "| PT | N | 36 |\n",
    "| PT | Y | 4 |\n",
    "\n",
    "To calculate the best split, we first calculate the Gini Index of the whole data:\n",
    "\n",
    "$$ Gini\\ Index\\ (GI) = 1 - Prop(Target=v_1)^2 - Prop(Target=v_2)^2 - ... - Prop(Target=v_n)^2 $$\n",
    "\n",
    "where *Prop* measures the proportion of observations with the specified target value:\n",
    "\n",
    "$$ Prop(Target=Value) = \\frac{Count(Target=Value)}{Count(Target)} $$\n",
    "\n",
    "In this example, we can calculate the Gini Index as $ Gini\\ Index = 1 - 0.05^2 - 0.95^2 $\n",
    "\n",
    "To calculate Gini Impurity, we use a combination of Gini Index calculations:\n",
    "\n",
    "$$ Impurity = GI(Target) - Prop(s_{left}) \\times GI(Target \\mid s_{left}) - Prop(s_{right}) \\times (Target \\mid s_{right}) $$\n",
    "\n",
    "For a split (s) on *Employment*, where the left node is *FT* and the right node is *PT*, the Gini Impurity is calculated as:\n",
    "\n",
    "$$ Impurity = 0.095 - 0.4 \\times (1 - \\frac{36}{40}^2 - \\frac{4}{40}^2) - 0.6 \\times (1 - \\frac{59}{60}^2 - \\frac{1}{60}^2) $$\n",
    "\n",
    "Impurity is calculated like this for all possible splits, and the split resulting is the smallest value is selected as the root node, producing a left and a right branch each with a corresponding subset of the original data. To complete our decision tree, we repeat the same calculations and continue to select splits on smaller and smaller subsets until reaching a stopping condition (such as reaching a maximum tree depth, or when $GI(Target) = 0$ (meaning all labels in the subset are equal).\n",
    "\n",
    "For features with multiple categories values, splits are typically tested against the various one-vs-rest combinations, and for features with continuous values, these values are typically binned into ranges for testing.\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "Trees resulting from the algorithm above are prone to becoming complex and specific to the training obversations, which means they will typically not generalise well. This is call overfitting - the resulting tree (or model) is over-fitted to our training data.\n",
    "\n",
    "We can detect overfitting by comparing the performance of the model on the obversations it was constructed on (training) and new data (testing). When the training error is less than the testing error, we know that the model is too specific to the training data and did not generalise well to the test data - and so it is overfitted.\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "as a method to combat overfitting while making the most of the data\n",
    "\n",
    "## Feature Reduction\n",
    "\n",
    "by determining most important variables from the tree structure\n",
    "\n",
    "## Implementing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Table of Contents](00.00-Learning-ML.ipynb#Table-of-Contents) &bull; [&larr; *Chapter 2.03 - k-Nearest Neighbours*](02.03-k-Nearest-Neighbours.ipynb) &bull; [*Chapter 2.05 - ?* &rarr;](02.05-?.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
