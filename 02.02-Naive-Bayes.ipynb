{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](00.00-Learning-ML.ipynb#Table-of-Contents) &bull; [&larr; *Chapter 2.01 - Dummy Classifiers*](02.01-Dummy-Classifiers.ipynb) &bull; [*Chapter 2.03 - ?* &rarr;](02.02-?.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "# 02.02 - Naive Bayes\n",
    "\n",
    "In statistics, *Bayes theorem* describes the probability of an occurence based on input conditions. The theorem states: the probability of A given B is equal to the probability of B given A multiplied by the probability of A and divided by the probability of B, or notationally:\n",
    "\n",
    "    P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "`P()` means *the probability of* and `|` means *given* or \"where\".\n",
    "\n",
    "A naive Bayes classifier applies this theorem naively, assuming that features (inputs into) in the model are indepedent of (unrelated to) each other.\n",
    "\n",
    "In the previous chapter, we looked at using class probabilities to build a dummy classifier, and considered an example that 95% of loans do not default. This probability is known as a *prior* probability - it is known without knowing anything about the class inputs.\n",
    "\n",
    "\n",
    "## Proof of Bayes theorem\n",
    "\n",
    "We can prove Bayes theorem by starting with the probability of two events, A and B, occuring together.\n",
    "\n",
    "    P(A and B) = P(A) * P(B|A)\n",
    "    also\n",
    "    P(A and B) = P(B) * P(A|B)\n",
    "\n",
    "Equating the right sides of each equation:\n",
    "\n",
    "    P(B) * P(A|B) = P(A) * P(B|A)\n",
    "\n",
    "Divide both sides by `P(B)`, gives us Bayes theorem:\n",
    "\n",
    "    P(A|B) = P(A) * P(B|A) / P(B)\n",
    "\n",
    "## Example\n",
    "\n",
    "Let's expand our dummy classifier example with some input:\n",
    "\n",
    "| Employment | Default | Count |\n",
    "|---|---|---|\n",
    "| FT | N | 59 |\n",
    "| FT | Y | 1 |\n",
    "| PT | N | 36 |\n",
    "| PT | Y | 4 |\n",
    "\n",
    "Probability of default given full-time employment:\n",
    "\n",
    "    P(Default=Y|Emp=FT) = P(Default=Y) * P(Emp=FT|Default=Y) / P(Emp=FT)\n",
    "\n",
    "    = 0.05 * 0.2 / 0.6\n",
    "    = 0.0167...\n",
    "\n",
    "Probability of default given part-time employment:\n",
    "\n",
    "    P(Default=Y|Emp=PT) = P(Default=Y) * P(Emp=PT|Default=Y) / P(Emp=PT)\n",
    "\n",
    "    = 0.05 * 0.8 / 0.4\n",
    "    = 0.1\n",
    "\n",
    "Given just one input, for this example we can see that part time employees are almost 6 times more likely to default than their full time counterparts.\n",
    "\n",
    "If we want to predict the class of a given employment type, we calculate the probability of all classes and take the maximum.\n",
    "\n",
    "To extend on the above, if the employment type is FT, we know the probability of default is 0.0167.\n",
    "\n",
    "The probability of not defaulting is:\n",
    "\n",
    "    P(Default=N|Emp=FT) = P(Default=N) * P(Emp=FT|Default=N) / P(Emp=FT)\n",
    "\n",
    "    = 0.95 * (59/95) / 0.6\n",
    "    = 0.983...\n",
    "\n",
    "Since there are only two classes of default (true or false), the probabilities are intuitively inverse! As you can see, a loan to a full time worker is predicted to not default.\n",
    "\n",
    "If we are not interested in the probability and only interested in the predicted class, we can take a shortcut and not calculate the divisor `P(Emp=FT)` for both equations, as it is the same for both - it can only scale the results.\n",
    "\n",
    "## What about multiple inputs?\n",
    "\n",
    "We can expand Bayes theorem with even more inputs and try to improve our classifier! This is where the naive aspect comes into play. For each input, we will assume (naively) that it is unrelated to every other input. Consider the following:\n",
    "\n",
    "| Gender | Employment | Default | Count |\n",
    "|---|---|---|---|\n",
    "| M | FT | N | 30 |\n",
    "| M | FT | Y | 1 |\n",
    "| M | PT | N | 14 |\n",
    "| M | PT | Y | 3 |\n",
    "| F | FT | N | 29 |\n",
    "| F | FT | Y | 0 |\n",
    "| F | PT | N | 22 |\n",
    "| F | PT | Y | 1 |\n",
    "\n",
    "While we won't go through the mathematical proof here, Bayes theorem is generalised for multiple inputs as:\n",
    "\n",
    "    p(class|f1,f2,f3,...) = p(class) * p(f1|class) * p(f2|class) * p(f3|class) ...\n",
    "    \n",
    "Let's predict default for a full-time employed female:\n",
    "\n",
    "    p(default=True|emp=FT,gen=F) = p(default=True) * p(emp=FT|default=True) * p(gen=F|default=True)\n",
    "    = 0.05 * 0.2 * 0.2\n",
    "    = 0.05\n",
    "    \n",
    "    p(default=False|emp=FT,gen=F) = p(default=False) * p(emp=FT|default=False) * p(gen=F|default=False)\n",
    "    = 0.95 * (59/95) * (51/95)\n",
    "    = 0.3167\n",
    "    \n",
    "Now was take the maximum of the two probabilities, and assign the corresponding class as our prediction. That is, for a full-time employed female, we predict no default.\n",
    "\n",
    "## What about numerical inputs?\n",
    "\n",
    "## What about multiple classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509203681473\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Table of Contents](00.00-Learning-ML.ipynb#Table-of-Contents) &bull; [&larr; *Chapter 2.01 - Dummy Classifiers*](02.01-Dummy-Classifiers.ipynb) &bull; [*Chapter 2.03 - ?* &rarr;](02.02-?.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
