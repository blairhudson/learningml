{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](00.00-Learning-ML.ipynb#Table-of-Contents) &bull; [&larr; *Chapter 2 - Classification*](02.00-Classification.ipynb) &bull; [*Chapter 2.02 - ?* &rarr;](02.02-?.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "# 02.01 - Starting Simple\n",
    "\n",
    "To really understand how classifiers work, we're going to start with two very basic models called dummy classifiers. \n",
    "\n",
    "Technically these aren't machine learning models, as they use simple rules defined by the user. As you will see, they provide a good baseline for performance and demonstrate the importance of using various performance measures to evaluate models. If you've ever witnessed someone *'wow'* an audience by describing a predictive model with an impressively high *accuracy* (such as 95%), you will see why this may not be as impressive as it sounds. (Accuracy has a special definition when we are talking about classification.)\n",
    "\n",
    "## Mode\n",
    "\n",
    "*Mode* is a statistical term for the most frequent value in a set of data. For example, in the set `a, b, b, b, c, d`, the value `b` occurs most frequently, so it is the mode. You can calculate the mode of a given dataset in Python with the `statistics.mode` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "data = ['a','b','b','b','c','d']\n",
    "\n",
    "mode(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example there are four unique values. In classification, when there are just two unique values in the labels, this is called *binary classification*. For example, consider the set `True, False, False, False, False`. The mode of this set is `False`, and in binary classification this is also known as the *majority class*.\n",
    "\n",
    "We can use the mode to create a very simple model for predicting a value (and without requiring any input). In binary classification, this is can be called a *majority class classifier*. Using the example above, a majority class classifier would always predict `False`, and given the example data it would achieve an **accuracy of 80%**! A great achievement for such a simple model.\n",
    "\n",
    "This terminology is potentially problematic. When talking about classification, accuracy is a measure of the proportion of predictions that are predicted correctly. The colloquial meaning of accuracy could mislead others about the performance of your model. Consider a set of 100 True and False labels that flag whether a loan has defaulted or not. Perhaps in this set, only 5 of the loans defaulted. With a basic model such as this, we could trivially achieve 95% accuracy by always predicting False.\n",
    "\n",
    "Using the table below, we can define some additional useful measures of performance:\n",
    "\n",
    "| | Predicted = True | Predicted = False |\n",
    "|---|---|---|\n",
    "| Actual = True | True Positive (TP)  | False Negative (FN) |\n",
    "| Actual = False | False Positive (FP) |  True Negative (TN) |\n",
    "\n",
    "*Accuracy* measures how often the model is correct, calculated as:\n",
    "> (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "*Sensitivity* (also called Recall or True Positive Rate) measures how often the model is correct when the value is actually true, calculated as:\n",
    "> TP / (FN + TP)\n",
    "\n",
    "*Fallout* (also called False Positive Rate) measures how often the model is incorrect with the value is actually false, calculated as:\n",
    "> FP / (TN + FP)\n",
    "\n",
    "*Precision* (also called Positive Predictive Value) measures the proportion of predictions are correct when the predicted value is True, calculated as:\n",
    "> TP / (FP + TP)\n",
    "\n",
    "Generally, the goal is to maximise accuracy, sensitivity and precision, and minimise fallout.\n",
    "\n",
    "Continuing with our loan defaults example above, let's calculate sensitivity and recall (remembering our model always predicts False):\n",
    "\n",
    "| | Predicted = True | Predicted = False |\n",
    "|---|---|---|\n",
    "| Actual = True | 0 (TP)  | 5 (FN) |\n",
    "| Actual = False | 0 (FP) |  95 (TN) |\n",
    "\n",
    "* We already know accuracy is 95%\n",
    "* Sensitivity = TP / (FN + TP) = 0 / (5 + 0) = 0%\n",
    "* Fallout = FP / (TN + FP) = 0 / (95 + 0) = 0%\n",
    "* Precision = TP / (FP + TP) = 0 / (0 + 0) = NaN\n",
    "\n",
    "Considering these additional metrics, we can now see that while the model accuracy is high, it's actually plain garbage for it's predicting loan defaults. ðŸ’©\n",
    "\n",
    "\n",
    "## Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Table of Contents](00.00-Learning-ML.ipynb#Table-of-Contents) &bull; [&larr; *Chapter 2 - Classification*](02.00-Classification.ipynb) &bull; [*Chapter 2.02 - ?* &rarr;](02.02-?.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
